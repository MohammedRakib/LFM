# Codes of Facilitating Multimodal Classification via Dynamically Learning Modality Gap


Here is the official PyTorch implementation of ''*Facilitating Multimodal Classification via Dynamically Learning Modality Gap*''

**Paper Title: "Facilitating Multimodal Classification via Dynamically Learning Modality Gap"**




## Code instruction

### Data Preparation
The original datasets can be found:
[CREMA-D](https://github.com/CheyneyComputerScience/CREMA-D),
[Kinetics-Sounds](https://github.com/cvdfoundation/kinetics-dataset),



### Run
The code use CREMA-D dataset as example. You can simply run the code using:  
<pre><code>
python Crema_ours.py
</code></pre>

For replication inquiries or issues, feel free to contact us via email at [wfq011207@163.com].